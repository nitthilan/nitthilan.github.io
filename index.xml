<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nitthilan</title>
    <link>https://nitthilan.github.io/</link>
      <atom:link href="https://nitthilan.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Nitthilan</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>nitthilan@gmail.com</copyright><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nitthilan.github.io/images/icon_hu794274cdbfab6a2e083ce074675524a2_29983_512x512_fill_lanczos_center_3.png</url>
      <title>Nitthilan</title>
      <link>https://nitthilan.github.io/</link>
    </image>
    
    <item>
      <title>Design and Optimization of Energy-Accuracy Tradeoff Networks for Mobile Platforms via Pretrained Deep Models</title>
      <link>https://nitthilan.github.io/publication/jayakodi-2020-design/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/jayakodi-2020-design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GRAMARCH: A GPU-ReRAM based Heterogeneous Architecture for Neural Image Segmentation</title>
      <link>https://nitthilan.github.io/publication/joardar-2020-gramarch/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/joardar-2020-gramarch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PETNet: Polycount and Energy Trade-off Deep Networks for Producing 3D Objects from Images</title>
      <link>https://nitthilan.github.io/publication/jayakodi-2020-petnet/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/jayakodi-2020-petnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uncertainty-Aware Search Framework for Multi-Objective Bayesian Optimization.</title>
      <link>https://nitthilan.github.io/publication/belakaria-2020-uncertainty/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/belakaria-2020-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active Anomaly Detection via Ensembles: Insights, Algorithms, and Interpretability</title>
      <link>https://nitthilan.github.io/publication/das-2019-active/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/das-2019-active/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MOOS: A multi-objective design space exploration and optimization framework for NoC enabled manycore systems</title>
      <link>https://nitthilan.github.io/publication/deshwal-2019-moos/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/deshwal-2019-moos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A Co-Design Approach</title>
      <link>https://nitthilan.github.io/publication/kannappan-2019-trading/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/kannappan-2019-trading/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active anomaly detection via ensembles</title>
      <link>https://nitthilan.github.io/publication/das-2018-active/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/das-2018-active/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trading-off accuracy and energy of deep inference on embedded systems: A co-design approach</title>
      <link>https://nitthilan.github.io/publication/jayakodi-2018-trading/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/publication/jayakodi-2018-trading/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://nitthilan.github.io/project_details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/project_details/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AI Resources (Under Development)</title>
      <link>https://nitthilan.github.io/ai_resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/ai_resource/</guid>
      <description>&lt;h6 id=&#34;sections&#34;&gt;Sections&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;#significant-areas&#34;&gt;Significant Areas&lt;/a&gt;-&lt;a href=&#34;#current-task-list&#34;&gt;Current Task List&lt;/a&gt; - &lt;a href=&#34;#tutorials&#34;&gt;Tutorials&lt;/a&gt; - &lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt; - &lt;a href=&#34;#prominent-papers&#34;&gt;Prominent Papers&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;multi-view-based-3d-models&#34;&gt;Multi view based 3D models&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Neural Fields Rendering System:
Requirements:
&lt;ul&gt;
&lt;li&gt;Store objects as neural network
&lt;ul&gt;
&lt;li&gt;It could be a single neural network for each object&lt;/li&gt;
&lt;li&gt;multiple obejcts encoded in the same network using embedding as input.&lt;/li&gt;
&lt;li&gt;Store embeedding for each voxel and a common neural network. A generalized marching cubes algorithm.&lt;/li&gt;
&lt;li&gt;A hybrid progressive representation
&lt;ul&gt;
&lt;li&gt;A global embedding for overall structure and progressive embedding for successive voxel breaking to refine the shape to increased quality&lt;/li&gt;
&lt;li&gt;DeepLS, SDF (It has a different embedding compared to NSVF). It tried to learn NN to learn smaller edges. Heirarchical NeRF -&amp;gt; NSVF&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can transformer networks be used instead of NN?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning material properties: &lt;a href=&#34;https://github.com/bmild/nerf/issues/23&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerf Issues&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Looks like there is a solution which uses Albedo and Illumination in CVPR. Environment maps are used to replicate theillumination problem&lt;/li&gt;
&lt;li&gt;Light could be used as a dot product i.e. a NN which takes the direction of light, direction of incident ray and material properties at that point and produces the output at that point&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;current-task-list&#34;&gt;Current Task List&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Modify NSVF - embedding and voxel culling&lt;/li&gt;
&lt;li&gt;Modify NeRF - to support SDF experiments, add support of embedding for individual shape using DeepSDF idea&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;list-of-ideas&#34;&gt;List of ideas:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SDF helps in physics based simulations i.e. they help in contact detection between objects and hence in object object touching etc - so may help in hair, cloth simulations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Experiment with Occupancy and SDF variations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can we split objects into separate objects which and then join them together later [Graffe paper]. Create a system which can align objects as different poses and translation and camera pose&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Nerf++ to encode forground and background object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NeRF for Human shapes: Fashion related human modeling design and animation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Garment Hair 3D geometry and charater animation.&lt;/li&gt;
&lt;li&gt;Nerf setup for hair and cloth simulation. Interpreenetation of hair strands into head and sholder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Learn a prior using NeRF and learn embeddings using a single input image [PiFU].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used trained NN and extract embeddings based on input image. Heirarchical NerF -&amp;gt; NSVF&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can Embedding be learnt for different parts like face, hands clothers. Can we use Pose as input - CPIGAN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using PSNR or SSIM to identify areas in the learnt model which are not properly learnt and using this guide the rays to be used for learning.
&lt;ul&gt;
&lt;li&gt;Create a weigting map of the areas which are lacking detail&lt;/li&gt;
&lt;li&gt;Sample rays based on this weighting map within a picture&lt;/li&gt;
&lt;li&gt;Sample rays based on weighting map of all the pictures&lt;/li&gt;
&lt;li&gt;Split voxels adaptively based on the PSNR and SSIM distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Progressively learning model using smaller images and gradually increasing the image dimension as we increase the model accuracy. The aim is to reduce the training time of the 3D model.
&lt;ul&gt;
&lt;li&gt;Estimating the bounding box automatically based on the camera position&lt;/li&gt;
&lt;li&gt;Use images from smaller size along widthxheight probably rations 2,4,8&lt;/li&gt;
&lt;li&gt;Select a small subset of camera postions to start with and gradually increase with iterations&lt;/li&gt;
&lt;li&gt;Allocate initial voxel size based on all the camera postions and interpixel ray distance and then as the image dimensions increase the voxel size would decrease accordingly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Culling Voxels - This reduces the number voxels used for representing the object. This helps in reduction of the memory .Voxels in the bounding box which are not touched by the rays. Two major regions
&lt;ul&gt;
&lt;li&gt;Voxels inside the object which are not reached by the rays&lt;/li&gt;
&lt;li&gt;Voxels outside the object region which are not touched by the training rays&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NeRF in Motion: Encoding motion for objects in a neural scene. There are diffferent ideas for it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Datasets: &lt;a href=&#34;https://arxiv.org/pdf/2011.13961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-NeRF: Neural Radiance Fields for Dynamic Scenes&lt;/a&gt; - Extends the dataset of NERF to dynamics, &lt;a href=&#34;https://zhan-xu.github.io/rig-net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RigNET&lt;/a&gt;, &lt;a href=&#34;https://graphics.tu-bs.de/people-snapshot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;People Snapshot Dataset&lt;/a&gt; - &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/wxu/VideoAvatar/content/video_shapes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video Based Reconstruction of 3D People Models&lt;/a&gt;, &lt;a href=&#34;http://buff.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BUFF: Bodies Under Flowing Fashion&lt;/a&gt; - &lt;a href=&#34;https://ps.is.tuebingen.mpg.de/publications/shape_under_cloth-cvpr17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detailed, accurate, human shape estimation from clothed {3D} scan sequences&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2004.00452.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PiFuHD&lt;/a&gt; - &lt;a href=&#34;https://renderpeople.com/free-3d-people/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;renderpeople&lt;/a&gt;, &lt;a href=&#34;https://hdrihaven.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HDRI Haven&lt;/a&gt;, &lt;a href=&#34;https://cape.is.tue.mpg.de/downloads&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cape&lt;/a&gt;, &lt;a href=&#34;https://github.com/QianliM/CAPE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://github.com/QianliM/cape_utils/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;utils&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Using normalized coordinate system i.e. map actual values to normalised value to then learn a warping function which adds on to it and then render it f(X(x), Y(y), Z(z)). Maps (x,y,z) for each time step. Learn how bones are mapped to mesh pixels. Find the transformation function) - Motion Capture based rendering system
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.12948.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deformable Neural Radiance Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.12950.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space-time Neural Irradiance Fields for Free-Viewpoint Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09790.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Radiance Flow for 4D View Synthesis and Video Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/nerfies-public/videos/nerfies_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deformable Neural Radiance Fields&lt;/a&gt; - &lt;a href=&#34;https://nerfies.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.13961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-NeRF: Neural Radiance Fields for Dynamic Scenes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.12490.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NSVF uses Hyper networks to encode every network encode for each time step. &lt;a href=&#34;https://arxiv.org/pdf/1906.01618.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SRN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Like mesh objects, can Bones, rigging and weighting be added for the objects thereby making it configurable &lt;a href=&#34;https://www.peachpit.com/articles/article.aspx?p=483793&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bone structure&lt;/a&gt;, &lt;a href=&#34;https://blog.machinimatrix.org/avastar/features/rigging-and-weighting/3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Embedding or Latent variable to control different aspects of a 3D generation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shape is encoded as a latent vector and then a shape with the (xyz) is used for predicting shape - &lt;a href=&#34;https://arxiv.org/pdf/2102.08860.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ShaRF: Shape-conditioned Radiance Fields from a Single View&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Conditioned on the image the generate embedding for each voxel which could be used as input along with xyz - &lt;a href=&#34;https://arxiv.org/pdf/2012.02190.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pixelNeRF: Neural Radiance Fields from One or Few Images&lt;/a&gt; - &lt;a href=&#34;https://github.com/sxyu/pixel-nerf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relighting of models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://xfields.mpi-inf.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation&lt;/a&gt;, &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/DeepRelightableTextures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Relightable Textures&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1904.12356.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deferred Neural Rendering: Image Synthesis using Neural Textures&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Few Shot Learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learning priors using bayesian neural networks: &lt;a href=&#34;https://en.wikipedia.org/wiki/Uncertainty_quantification&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Uncertainity Quantification&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1505.05424&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weight Uncertainity in NN&lt;/a&gt;, &lt;a href=&#34;http://krasserm.github.io/2019/03/14/bayesian-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Variational Inference in Bayesian NN&lt;/a&gt; - The idea is to train priors of shapes using neural networks and then try to learn the representation using a single image input. The prior stores range of uncertainity in the variance of the weights and tries to find the right instance value using a single image input. We try to replace a NN in NeRF to a Baysian NN and try to model the uncertainity in shape as the weights of the bayesian-neural-networks&lt;/li&gt;
&lt;li&gt;Understanding GPT3: &lt;a href=&#34;https://arxiv.org/pdf/2005.14165.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT2 Blog&lt;/a&gt;, &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT2 Paper&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=_8yVOC4ciXc&amp;amp;ab_channel=Computerphile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=SY5PvZrJhLE&amp;amp;ab_channel=YannicKilcher&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube2&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Transformers for 3d model - &lt;a href=&#34;https://papers.nips.cc/paper/6206-perspective-transformer-nets-learning-single-view-3d-object-reconstruction-without-3d-supervision.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perspective Transformer Nets&lt;/a&gt;, &lt;a href=&#34;http://proceedings.mlr.press/v97/lee19d/lee19d.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Set Transformer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1906.10887.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatial Transformer for 3D Point Clouds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://leggedrobotics.github.io/rl-blindloco/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Possible continous learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Active learning for 3d object reconstruction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Structure from motion using deep learning: &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/1998/Fitzgibbon98a/fitzgibbon98a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Turntable&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can NeRF be modeled to run without the camera parameters? Since we are modelling the neural network as a funtioin of x,y,z can we learn using SGD the model without the camera parameters?&lt;/li&gt;
&lt;li&gt;Fix one camera position. Model the 3D model as function of relative camera parrameter. Try minimising the error of images while we learn the error from different projections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Splitting light, view and time and directly rendering 2D images - X-field - &lt;a href=&#34;https://arxiv.org/pdf/2010.00450.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/m-bemana/xfields&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compress 3D shape representation by using entries from a codebook like the idea in VQVAE - &lt;a href=&#34;https://arxiv.org/pdf/1711.00937.pdf,&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1711.00937.pdf,&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Depth/Height map as implicit functions :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using a base template and encoding the shape as a structure over the template - &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Shape Templates with Structured Implicit Functions&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/ldif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Human models: Converting 3d model to 2d surface plane - developable surface SMPL
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DensePose/issues/116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMPL to UV mapping&lt;/a&gt;, &lt;a href=&#34;https://github.com/Lotayou/densebody_pytorch/issues/41&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UV map to SMPL model&lt;/a&gt;, &lt;a href=&#34;https://github.com/CalciferZh/SMPL/issues/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to get UV coordinates for the template&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vchoutas/smplify-x/blob/master/smplifyx/fitting.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fitting code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@apofeniaco/p3d-body-detection-with-smplify-x-ced6c38871df&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tutorial&lt;/a&gt;, &lt;a href=&#34;https://github.com/ortegatron/playing_smplifyx/tree/master/smplifyx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code sample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenPose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/zkatuv5shs8d4tlwr8ecc/Change-parameters-to-new-coordinate-system.paper?dl=0&amp;amp;rlkey=lotq1sh6wzkmyttisc05h0in0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Converting from local to global&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://khanhha.github.io/posts/SMPL-model-introduction/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding SMPL&lt;/a&gt;, &lt;a href=&#34;https://github.com/vchoutas/smplx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMPLX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vchoutas/smplx/tree/master/transfer_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transfering between SMPL-SMPLX-SMPLH-FLAME-MANO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SMPL - &lt;a href=&#34;http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers&lt;/a&gt;, &lt;a href=&#34;https://smpl.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt; - SMPLX - &lt;a href=&#34;https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/497/SMPL-X.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/498/SMPL-X-supp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;supp&lt;/a&gt;, &lt;a href=&#34;https://smpl-x.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Face Fitting - &lt;a href=&#34;https://github.com/Rubikplayer/flame-fitting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flame fitting&lt;/a&gt;, &lt;a href=&#34;https://flame.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FLAME&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Approximating 3d shapes with blobs/ellipsoids - &lt;a href=&#34;https://github.com/google/ldif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1912.06126.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Local Deep Implicit Functions for 3D Shape&lt;/a&gt;, &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Shape Templates with Structured Implicit Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Online 3d models - Dataset - &lt;a href=&#34;https://www.turbosquid.com/3d-model/chair&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Turbosquid&lt;/a&gt;, &lt;a href=&#34;https://sketchfab.com/tags/freemodel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sketchfab&lt;/a&gt;, &lt;a href=&#34;http://pixologic.com/turntable/?page=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pixologic, Zbrush&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Synching two cameras - &lt;a href=&#34;https://github.com/google-research/libsoftwaresync&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;libsoftwaresync&lt;/a&gt; -
Code list:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/shunsukesaito/PIFu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/shunsukesaito/PIFu&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/pifuhd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/facebookresearch/pifuhd&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/kwea123/nerf_pl&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/NSVF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/facebookresearch/NSVF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nerf Universe:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Basic Nerf&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ability to choose appropriate texture and material properties based on BRDF&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;prominent-papers&#34;&gt;Prominent Papers&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/list/cs.CV/pastweek?show=490&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arxiv - Computer Vision&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/scholar?cites=9378169911033868166&amp;amp;as_sdt=5,48&amp;amp;sciodt=0,48&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerf - citations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/scholar?cites=8122086353742917335&amp;amp;as_sdt=5,48&amp;amp;sciodt=0,48&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NSVF - citations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://dellaert.github.io/NeRF/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeRF Explosion 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome NeRF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis: &lt;a href=&#34;https://arxiv.org/pdf/2003.08934.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123/nerf_pl/blob/master/README_mesh.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mesh reconstruction&lt;/a&gt;, &lt;a href=&#34;https://github.com/bmild/nerf/issues/44&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;color reproduction&lt;/a&gt;, &lt;a href=&#34;https://github.com/pmneila/PyMCubes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMcubes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Sparse Voxel Fields: &lt;a href=&#34;https://arxiv.org/pdf/2007.11571.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains: &lt;a href=&#34;https://arxiv.org/pdf/2006.10739.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generating Diverse High-Fidelity Images with VQ-VAE-2: &lt;a href=&#34;https://arxiv.org/pdf/1906.00446.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections: &lt;a href=&#34;https://arxiv.org/pdf/2008.02268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Rendering: &lt;a href=&#34;https://github.com/weihaox/awesome-neural-rendering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Kai-46/nerfplusplus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerf++&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/2010.07492.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.07064.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iNerf&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2102.07064.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerf&amp;ndash;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://research.fb.com/wp-content/uploads/2020/05/FroDO-From-Detections-to-3D-Objects.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FroDo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.00452.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PIFuHD&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.05172.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PIFu&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.12100.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRAF&lt;/a&gt; - &lt;a href=&#34;https://avg.is.tuebingen.mpg.de/publications/schwarz2020neurips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt; - &lt;a href=&#34;https://github.com/autonomousvision/graf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimizing the Latent Space of Generative Networks&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://demuc.de/papers/schoenberger2016sfm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SFM - Structure from motion&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.unc.edu/~ezheng/resources/mvs_2016/eccv2016.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pixelwise View Selection for Unstructured Multi-View Stereo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colmap.github.io/tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLMAP Documentation&lt;/a&gt; - &lt;a href=&#34;https://colmap.github.io/faq.html#reconstruct-sparse-dense-model-from-known-camera-poses&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dense Reconstruction - multi-view&lt;/a&gt; - &lt;a href=&#34;https://colmap.github.io/faq.html#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Structure From Motion - &lt;a href=&#34;http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FISHER/RANSAC/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RANSAC&lt;/a&gt;, &lt;a href=&#34;http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RANSAC1&lt;/a&gt;, &lt;a href=&#34;https://www.rose-hulman.edu/class/cs/csse461/handouts/Day37/SINGLES2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;five-point relative pose problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colmap.github.io/tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;undistort images - making lines straight - usually lens causes distortion - &lt;a href=&#34;https://stackoverflow.com/questions/39530110/camera-calibration-for-structure-from-motion-with-opencv-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;camera calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;patchmatch stereo algorithm - &lt;a href=&#34;http://www.bmva.org/bmvc/2011/proceedings/paper14/paper14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stereo Matching&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/PatchMatch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki&lt;/a&gt;, &lt;a href=&#34;https://github.com/ivanbergonzani/patch-match-stereo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;stereo_fusion algorithms - &lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall07/cos429/slides/stereo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poisson mesh reconstruction - &lt;a href=&#34;http://hhoppe.com/poissonrecon.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computer Vision Course &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019&lt;/a&gt; - &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec13_alignment.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;13 Alignment&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec14_calibration.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;14 Calibration&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec15_single_view.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;15 Single View&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec16_epipolar.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;16 Epipolar&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec17_sfm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;17 SFM&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec18_stereo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;18 Stereo&lt;/a&gt;, &lt;a href=&#34;https://slazebni.cs.illinois.edu/spring19/lec19_multiview_stereo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;19 Multiview stereo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSDF&lt;/a&gt; - &lt;a href=&#34;https://github.com/facebookresearch/DeepSDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt; - &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Park_DeepSDF_Learning_Continuous_CVPR_2019_supplemental.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Supp&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.02268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerf-W&lt;/a&gt; - Nerf in the wild&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2019/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.03805.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State of the Art on Neural Rendering&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google/ldif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Local Deep Implicit Functions for 3D Shape&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.13650.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.07233.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable View Synthesis&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://vladlen.info/papers/FVS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Free View Synthesis&lt;/a&gt; - &lt;a href=&#34;https://github.com/intel-isl/FreeViewSynthesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.09662.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MetaSDF: Meta-learning Signed Distance Functions&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.umd.edu/~yuejiang/papers/SDFDiff.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization&lt;/a&gt; - &lt;a href=&#34;https://github.com/YueJiang-nj/CVPR2020-SDFDiff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/spring14/cos426/lectures/09-implicit.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Implicit function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Rd0nBO6--bM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vladlen Koltun: Towards Photorealism&lt;/a&gt;, &lt;a href=&#34;http://vladlen.info/contact/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contact&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Understanding illumination: Differenctiable renderer idea, &lt;a href=&#34;https://arxiv.org/pdf/2004.00403.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two-shot Spatially-varying BRDF and Shape Estimation&lt;/a&gt; &lt;a href=&#34;https://github.com/NVlabs/two-shot-brdf-shape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deep Learning Papers: &lt;a href=&#34;https://arxiv.org/pdf/2012.03918.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeRD: Neural Reflectance Decomposition from Image Collections&lt;/a&gt; - &lt;a href=&#34;https://markboss.me/publication/2021-nerd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2012.03927.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt; - &lt;a href=&#34;https://pratulsrinivasan.github.io/nerv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tutorial on Spherical Gaussians - &lt;a href=&#34;https://mynameismjp.wordpress.com/2016/10/09/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SG SERIES PART 1-6&lt;/a&gt;, &lt;a href=&#34;http://www.cse.chalmers.se/~uffe/xjobb/Readings/GlobalIllumination/Spherical%20Harmonic%20Lighting%20-%20the%20gritty%20details.pdfs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spherical Harmonic Lighting: The Gritty Details&lt;/a&gt;, &lt;a href=&#34;http://www.codinglabs.net/article_physically_based_rendering_cook_torrance.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cook Torrence Model&lt;/a&gt;, &lt;a href=&#34;https://cseweb.ucsd.edu/classes/wi18/cse167-a/lec13.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Environment mapping Slides&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://inst.eecs.berkeley.edu/~cs294-13/fa09/lectures/cookpaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Reflectance Model for Computer Graphics&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bidirectional reflectance distribution function - BRDF&lt;/a&gt; - &lt;a href=&#34;https://en.wikipedia.org/wiki/Phong_reflection_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phong Refelection model&lt;/a&gt; - &lt;a href=&#34;https://en.wikipedia.org/wiki/Specular_highlight#Cook%E2%80%93Torrance_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cookâ€“Torrance model&lt;/a&gt;, &lt;a href=&#34;https://static1.squarespace.com/static/58586fa5ebbd1a60e7d76d3e/t/593a3afa46c3c4a376d779f6/1496988449807/s2012_pbs_disney_brdf_notes_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Physically-Based Shading at Disney&lt;/a&gt;, &lt;a href=&#34;https://www.alexandre-pestana.com/disney-principled-brdf-implementation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disney BRDF Base color Metallic parametrization&lt;/a&gt;, &lt;a href=&#34;https://cseweb.ucsd.edu/~ravir/papers/envmap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Efficient Representation for Irradiance Environment Maps&lt;/a&gt;, &lt;a href=&#34;https://cseweb.ucsd.edu/~ravir/papers/invlamb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Relationship between Radiance and Irradiance: Determining the illumination from images of a convex Lambertian object&lt;/a&gt;, &lt;a href=&#34;http://www.codinglabs.net/article_physically_based_rendering.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Physically based rendering (PBR)&lt;/a&gt;, &lt;a href=&#34;https://learnopengl.com/PBR/Theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenGL PBR&lt;/a&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Material Editing - &lt;a href=&#34;https://users.cg.tuwien.ac.at/~zsolnai/wp/wp-content/uploads/2019/09/pme.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Photorealistic Material Editing Through Direct Image Manipulation&lt;/a&gt;, &lt;a href=&#34;https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian Material Synthesis&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not Read, Probably good:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.03824.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Reflectance Fields for Appearance Acquisition&lt;/a&gt; - &lt;a href=&#34;http://cseweb.ucsd.edu/~bisai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;author&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/12/sg.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All-Frequency Rendering of Dynamic, Spatially-Varying Reflectance&lt;/a&gt;, &lt;a href=&#34;https://www.slideserve.com/calder/all-frequency-rendering-of-dynamic-spatially-varying-reflectance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial slide&lt;/a&gt; - tutorials on using Spherical Gaussians for BRDF and Environment illumination - &lt;a href=&#34;https://arxiv.org/pdf/2012.05903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Portrait Neural Radiance Fields from a Single Image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/afs/cs/academic/class/16823-s16/www/pdfs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Physics based Methods in Vision&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://dreamfarmstudios.com/blog/the-ultimate-guide-to-lighting-fundamentals-for-3d/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ligting basics&lt;/a&gt;, &lt;a href=&#34;https://www.facebook.com/BusinessInsiderIndia/videos/366771777864939&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video&lt;/a&gt;, &lt;a href=&#34;https://dreamfarmstudios.com/blog/getting-to-know-3d-texturing-in-animation-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3D Texturing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Online 3D model assets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.turbosquid.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TurboSquid&lt;/a&gt; - Using professional models. This is a good reference&lt;/li&gt;
&lt;li&gt;Environmental Maps - &lt;a href=&#34;https://hdrihaven.com/hdris/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HDRI Haven&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Depth Estimation from input image: &lt;a href=&#34;https://arxiv.org/pdf/1907.01341v3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiDaS&lt;/a&gt;, &lt;a href=&#34;https://github.com/intel-isl/MiDaS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;,&lt;a href=&#34;https://pytorch.org/hub/intelisl_midas_v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;latest code&lt;/a&gt; - Perpetual View Generation - &lt;a href=&#34;https://infinite-nature.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Infinite Nature&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://facebookresearch.github.io/one_shot_3d_photography/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;One Shot 3D Photography&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/one_shot_3d_photography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local Deep Implicit Functions for 3D Shape - &lt;a href=&#34;https://github.com/google/ldif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1912.06126.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; - Learning a transformation matrix to approximate a gaussian function to define an implicit function. &lt;a href=&#34;https://arxiv.org/pdf/1904.06447.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Shape Templates with Structured Implicit Functions&lt;/a&gt; - Can we use this to approximate shapes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Understanding SDF based operations - ray marching and sdf - &lt;a href=&#34;http://jamie-wong.com/2016/07/15/ray-marching-signed-distance-functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article1&lt;/a&gt;, &lt;a href=&#34;https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article2&lt;/a&gt;, &lt;a href=&#34;https://www.cl.cam.ac.uk/teaching/1819/FGraphics/1.%20Ray%20Marching%20and%20Signed%20Distance%20Fields.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDF renderiing&lt;/a&gt;, &lt;a href=&#34;http://graphics.stanford.edu/courses/cs348b-20-spring-content/uploads/hart.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphere Tracing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;canonical coordinates 3d model, quaternion representation of rotation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adding secondary motion - &lt;a href=&#34;https://www.dgp.toronto.edu/projects/complementary-dynamics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complementary Dynamics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not read: &lt;a href=&#34;https://arxiv.org/pdf/2012.09365.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Recover 3D Scene Shape from a Single Image&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2012.06434.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2012.08503.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object-Centric Neural Scene Rendering&lt;/a&gt;, &lt;a href=&#34;https://capworkshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computer assisted prog&lt;/a&gt;, &lt;a href=&#34;https://github.com/chenhsuanlin/signed-distance-SRN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;signed-distance-SRN&lt;/a&gt;, &lt;a href=&#34;https://github.com/autonomousvision/differentiable_volumetric_rendering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;differentiable_volumetric_rendering&lt;/a&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quaternions - &lt;a href=&#34;https://jinyongjeong.github.io/Download/SE3/jlblanco2010geometry3d_techrep.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ref1&lt;/a&gt;, &lt;a href=&#34;https://maxime-tournier.github.io/notes/quaternions.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ref2&lt;/a&gt;, &lt;a href=&#34;https://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SE(3) transform&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.04948&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StyleGAN&lt;/a&gt;, &lt;a href=&#34;https://rameenabdal.github.io/StyleFlow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StyleFlow&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.07064.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeRFâˆ’âˆ’: Neural Radiance Fields Without Known Camera Parameters&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Companies - &lt;a href=&#34;https://www.teriflix.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Teriflix&lt;/a&gt;, &lt;a href=&#34;https://www.artella.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Searching for artist&lt;/a&gt;, &lt;a href=&#34;https://paperboatstudios.co/index&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paperboatstudios&lt;/a&gt;, &lt;a href=&#34;https://www.studiodurga.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;studiodurga&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI Sculptor: Assuming the rays as a nail, we like to sculpt a 3D model based on multiple input views. We use the NSVF as our based code. The list of problems we are planning to attack:
- Marching cubes would fail to recover the surface since the function learns only the boundary and does not learn inside the object. So a different learning algorithm is required for learning the 3D mesh object. We need to find a better algorithm to extract the 3D mesh and surface information.
- A soluttion would be to shoot rays from different directions and choose only those points which have normals which are parallel to the ray of intersection. Use these points as an input to creat a point cloud and then convert this point cloud to a mesh using &lt;a href=&#34;https://towardsdatascience.com/5-step-guide-to-generate-3d-meshes-from-point-clouds-with-python-36bad397d8ba&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pointcloud2mesh&lt;/a&gt; algorithms, &lt;a href=&#34;https://www.cgal.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Surface Reconstruction&lt;/a&gt;
- Alternative representation: Signed Distance field representation has better representation of objects than transparency representation. Can we use SDF instead of transparency? Define a rendering function using SDF. Use this function instead of transparency based rendering function and then use it to represent Neural scene &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSDF&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2003.10983.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepLS&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1802.05384.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papier-MachË†e&lt;/a&gt;, &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Occupancy Networks&lt;/a&gt;
- Heirarchical representation: Nerf/DeepSDF use NN to represent the whole scene. NSVF/DeepLS use local embedding information to represent shapes in voxels. Can a heirarchchical representation of latent variables be used to represnet the whole shape to get consistent representation.
- Can the shape information used as code in DeepSDF be used in NERF/NSVF kind of setup to reduce the number of images to encode a scene directly from a siingle image
- Physics simulation using NeRF: &lt;a href=&#34;https://users.cg.tuwien.ac.at/zsolnai/gfx/fluid_control_msc_thesis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Real Time Fluid simulation&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2002.09405.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to simulate&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;list-of-questions&#34;&gt;List of questions&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;multiresolution in signed distance function, how are multiple objects composed using SDF, What is the maximum capacity of NN?, occupancy vs sdf comparison&lt;/li&gt;
&lt;li&gt;a KD-tree - nearest neighbor algorithm -&lt;/li&gt;
&lt;li&gt;distance transform for any watertight shapes&lt;/li&gt;
&lt;li&gt;Weight normalization: A simple reparameterization to accelerate training of deep neural networks. - speed up training instead of batch normalization&lt;/li&gt;
&lt;li&gt;H.P.: Multi-level partition of unity implicits. - Splitting a volume into smaller regions and trying to generate to whole shape based on sum of smaller regions - weighting function is 1 when you are within the region - &lt;a href=&#34;https://www.cc.gatech.edu/~turk/my_papers/mpu_implicits.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/~turk/my_papers/mpu_implicits.pdf&lt;/a&gt; - &lt;a href=&#34;https://en.wikipedia.org/wiki/Partition_of_unity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Partition_of_unity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;datasets: &lt;a href=&#34;https://3dwarehouse.sketchup.com/,&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://3dwarehouse.sketchup.com/,&lt;/a&gt; &lt;a href=&#34;http://graphics.stanford.edu/data/3Dscanrep/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://graphics.stanford.edu/data/3Dscanrep/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;other-queries&#34;&gt;Other queries&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.einstein.ai/the-ai-economist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Economist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/dall-e/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DALLÂ·E: Creating Images from Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://thegradient.pub/transformers-are-graph-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers are Graph Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/immersive-lf-video-siggraph2020/ImmersiveLightFieldVideoWithALayeredMeshRepresentation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Immersive Light Field Video with a Layered Mesh Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.12631.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EFFICIENT CONTINUAL LEARNING WITH MODULAR NETWORKS AND TASK-DRIVEN PRIORS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.12477.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IIRC: Incremental Implicitly-Refined Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A algorithm to solve generic problems: &lt;a href=&#34;https://xinyazhang.gitlab.io/puzzletunneldiscovery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C-Space Tunnel Discovery for Puzzle Path Planning&lt;/a&gt;, &lt;a href=&#34;https://gitlab.com/xinyazhang/puzzle-geometry/-/tree/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;More Puzzles&lt;/a&gt;, &lt;a href=&#34;https://xinyazhang.gitlab.io/puzzletunneldiscovery/assets/MainPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; - Concentrate on the different planning algorithms. The baisc idea would be to find tunnels (i.e. narrow points which could possibly leead to a solution). Then have a duble tree starting from Start and reversely from goal. This could be done for every tunnel point. This stage is the blooming stage. You could approximate the value of the tree using neural networks. Try finding links from the bloomed tree and the successive tunnel paths. IF we find a route find the optimal route from start to end - Probabilistic Roadmap Path Planning &lt;a href=&#34;http://www.cs.columbia.edu/~allen/F15/NOTES/Probabilisticpath.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRM Planner&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/MotionPlanning-v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRM1&lt;/a&gt;, &lt;a href=&#34;http://planning.cs.uiuc.edu/node772.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RDT-Based Methods&lt;/a&gt; - dual-tree RDT algorithm&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cg.informatik.uni-freiburg.de/publications/2019_TOG_strongCoupling.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interlinked SPH Pressure Solvers for Strong Fluid-Rigid Coupling&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;significant-areas&#34;&gt;Significant Areas&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian Optimzation based GAN latent variable search&lt;/li&gt;
&lt;li&gt;Bayesian Neural network based BO&lt;/li&gt;
&lt;li&gt;StyleGAN&lt;/li&gt;
&lt;li&gt;Camera pose estimate&lt;/li&gt;
&lt;li&gt;Fourier feature networks&lt;/li&gt;
&lt;li&gt;Photogrammetry - &lt;a href=&#34;https://all3dp.com/1/best-photogrammetry-software/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part segmentation&lt;/li&gt;
&lt;li&gt;HairNet&lt;/li&gt;
&lt;li&gt;Sculpting&lt;/li&gt;
&lt;li&gt;SMPL&lt;/li&gt;
&lt;li&gt;Image Harmonization&lt;/li&gt;
&lt;li&gt;Pose Transfer&lt;/li&gt;
&lt;li&gt;Multi-view rendering&lt;/li&gt;
&lt;li&gt;HyperNetworks&lt;/li&gt;
&lt;li&gt;Continous Learning - &lt;a href=&#34;https://imirzadeh.me/#contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Iman&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2006.06958.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0893608019300231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Review Paper&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2007.00487.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tackling catastropic forgetting&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2004.00713.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Incremental learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/timzhang642/3D-Machine-Learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3D Machine Learning Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;to-be-read-papers&#34;&gt;To Be Read Papers&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ondrejtexler.github.io/patch-based_training/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Video StylizationInteractive Video Stylization&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2010.05334.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resolution Dependent GAN&lt;/a&gt;, &lt;a href=&#34;http://chengao.vision/FGVC/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video Completion&lt;/a&gt;,&lt;a href=&#34;https://arxiv.org/pdf/1707.05776.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latent Space of Generative Networks&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1703.04977.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Deep Learning for Computer Vision&lt;/a&gt;, &lt;a href=&#34;https://genforce.github.io/idinvert/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Real Image Editing GAN&lt;/a&gt;, &lt;a href=&#34;https://rewriting.csail.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rewriting a Deep Generative Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Softwares: &lt;a href=&#34;https://www.synthesia.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Vdo&lt;/a&gt;, &lt;a href=&#34;https://www.cinetracer.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CineTracer&lt;/a&gt;,&lt;a href=&#34;https://vast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPU Sharing&lt;/a&gt;,&lt;a href=&#34;https://www.spyfu.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SpyFU&lt;/a&gt;, &lt;a href=&#34;https://obsproject.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OBS Studio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.fb.com/wp-content/uploads/2020/06/Neural-Supersampling-for-Real-time-Rendering.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;current-task-list-1&#34;&gt;Current Task List&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;GAN basics,&lt;/li&gt;
&lt;li&gt;Running models on GPU&lt;/li&gt;
&lt;li&gt;Framework - Pytorch, Keras, numpy, Pytorch3D&lt;/li&gt;
&lt;li&gt;AlphaZero,&lt;/li&gt;
&lt;li&gt;CPISADGAN&lt;/li&gt;
&lt;li&gt;BO&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DeepRobot2020/books/blob/master/Multiple%20View%20Geometry%20in%20Computer%20Vision%20%28Second%20Edition%29.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MultiView Geometry&lt;/a&gt;, &lt;a href=&#34;http://ksimek.github.io/2012/08/14/decompose/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Camera Basics&lt;/a&gt;, &lt;a href=&#34;https://mathworld.wolfram.com/RotationMatrix.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotation Matrix&lt;/a&gt;, &lt;a href=&#34;http://robotics.stanford.edu/~birch/projective/node2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Projective, Affine, Euclidean Geometry&lt;/a&gt;, &lt;a href=&#34;https://learnopengl.com/Getting-started/Coordinate-Systems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NDC Co-ordinate Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;applications&#34;&gt;Applications&lt;/h6&gt;
&lt;h6 id=&#34;graphics-related&#34;&gt;Graphics related&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/design-visualization/omniverse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Omniverse&lt;/a&gt;, &lt;a href=&#34;https://mmacklin.com/sdfcontact.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Signed Distance Field Collision&lt;/a&gt;, &lt;a href=&#34;http://rgl.epfl.ch/publications/Zeltner2020Specular&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Specular Manifold Sampling&lt;/a&gt;, &lt;a href=&#34;https://matthias-research.github.io/pages/publications/PBDBodies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Detailed Rigid Body Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Character creation: &lt;a href=&#34;https://www.reallusion.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reallusion&lt;/a&gt;, &lt;a href=&#34;https://www.mixamo.com/fuse/1.3/eol&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adobe Fuse/Mixamo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sections-1&#34;&gt;Sections&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;#children-resources&#34;&gt;Children resources&lt;/a&gt; - &lt;a href=&#34;#videos&#34;&gt;Videos&lt;/a&gt; - &lt;a href=&#34;#accelerators&#34;&gt;Accelerators&lt;/a&gt; - &lt;a href=&#34;#channels-followed&#34;&gt;Channels followed&lt;/a&gt; - &lt;a href=&#34;#gpu-and-cuda-resource&#34;&gt;GPU and CUDA resource&lt;/a&gt; - &lt;a href=&#34;#projects&#34;&gt;Projects&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;tutorials-1&#34;&gt;Tutorials&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aipaygrad.es/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI pay grades&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;children-resources&#34;&gt;Children resources&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ecraft2learn.github.io/ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ecraft2learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.media.mit.edu/posts/kids-ai-devices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningforkids.co.uk/#!/links#top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;machinelearningforkids&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;videos&#34;&gt;Videos&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dmkPJpWCVcI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Heroes of Deep Learning: Andrew Ng interviews Pieter Abbeel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=St5lxIxYGkI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepmind opensources starcraft II ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cLC_GHZCOVQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI bots beats DOTA Worl champion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_aUq7lmMfxo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepmind and UCL course work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;list-of-topics-to-be-covered&#34;&gt;List of topics to be covered&lt;/h6&gt;
&lt;h6 id=&#34;good-resources&#34;&gt;Good resources&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/sutton/book/the-book-2nd.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sutton Book&lt;/a&gt;: &lt;a href=&#34;http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Book Draft&lt;/a&gt;, &lt;a href=&#34;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code And Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2pWv7GOvuf0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Mind: RL Course by David Silver&lt;/a&gt; - First of a 10 lecture series - &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;accelerators&#34;&gt;Accelerators&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://betalist.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;betalist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://angel.co/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;angel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.crunchbase.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;crunchbase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.producthunt.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;producthunt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.startupschool.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;startupschool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://disneyaccelerator.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;disneyaccelerator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;job-options&#34;&gt;Job Options&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://e3expo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;E3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cvpr2020.thecvf.com/jobs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVPR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://s2020.siggraph.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Siggraph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ctnanimationexpo.com/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cartoon network animation expo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;internships&#34;&gt;Internships&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Studio list - Pixar, nickelodean, laika, dreamworks, bluesky, warner brothers, cartoon network, Blizzard&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=44DdiybwJ8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to get a pixar internship&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.animationcareerreview.com/articles/walt-disney-animation-studios-career-profile#:~:text=According%20to%20the%20company%20website,Maya%20or%20a%20similar%20program.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Animation colleges&lt;/a&gt; - &lt;a href=&#34;http://conceptdesignacad.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conceptdesignacad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dY-Ts69F4Xo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to get a job at Blizzard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;portfolio-creation&#34;&gt;Portfolio creation&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deviantart.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DevianArt&lt;/a&gt;, Instagram, YouTube&lt;/li&gt;
&lt;li&gt;List of inspiring artists: bobby pontillas, rafael grassetti &lt;a href=&#34;https://gumroad.com/grassettiart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;human anatomy&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=VmUikhiWu8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube&lt;/a&gt;, Laura Price &lt;a href=&#34;https://www.youtube.com/watch?v=lfZ8HKUhxak&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MY ART JOURNEY&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=dY-Ts69F4Xo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Vicente&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/channel/UCLEVrhumRsK67JkP3G4w5cQ/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ross draws&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://polycount.com/forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Polycount&lt;/a&gt;, &lt;a href=&#34;https://www.zbrushcentral.com/t/gallery/227023&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zbrush central&lt;/a&gt;, &lt;a href=&#34;https://www.artstation.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;artstation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Books - &lt;a href=&#34;https://www.amazon.com/dp/B00LKL6Q3S/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Composing Pictures&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Illusion-Life-Disney-Animation/dp/0786860707&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;illusion of life&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Animators-Survival-Kit-Expanded-Principles/dp/0571238343&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Animators survival kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;channels-followed&#34;&gt;Channels followed&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://deepai.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepAI: The front page of A.I.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;gpu-and-cuda-resource&#34;&gt;GPU and CUDA resource&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;p2 - NVIDIA Tesla K80 - 2 x 2496&lt;/li&gt;
&lt;li&gt;G3 - NVIDIA Tesla M60 - 4096 NVIDIA CUDAÂ® cores (2048 per GPU)&lt;/li&gt;
&lt;li&gt;G2 - NVIDIA GRID K520 GPUs - 3072 core&lt;/li&gt;
&lt;li&gt;CG1 -  NVIDIA Tesla M2050&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/academic_gpu_seeding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA Grant&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVIDIA Quadro M5000 - 2048 [GeForce Titan Xp, Quadro P5000, NVIDIA M5000 specification]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GTX 1080i&lt;/a&gt; - 3584 - 700$&lt;/li&gt;
&lt;li&gt;GTX 1080 - 2,560 - 500$&lt;/li&gt;
&lt;li&gt;GTX 1070 - 1920 - 390&lt;/li&gt;
&lt;li&gt;Titan X pascal - 3584&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.phoronix.com/scan.php?page=article&amp;amp;item=jetson-tegra-x2&amp;amp;num=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jetson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EGPU: &lt;a href=&#34;https://egpu.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;egpu&lt;/a&gt;, &lt;a href=&#34;https://9to5mac.com/2017/04/11/hands-on-powering-the-macbook-pro-with-an-egpu-using-nvidias-new-pascal-drivers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;9to5mac&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Fun</title>
      <link>https://nitthilan.github.io/entertainment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/entertainment/</guid>
      <description>&lt;h3 id=&#34;sections&#34;&gt;Sections&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;#drawing&#34;&gt;Drawing&lt;/a&gt; - &lt;a href=&#34;#movies&#34;&gt;Movies&lt;/a&gt; - &lt;a href=&#34;#short-stories-tamil&#34;&gt;Short Stories (Tamil)&lt;/a&gt; - &lt;a href=&#34;#investment&#34;&gt;Investment&lt;/a&gt; - &lt;a href=&#34;#cooking&#34;&gt;Cooking&lt;/a&gt; - &lt;a href=&#34;#writing&#34;&gt;Writing&lt;/a&gt; - &lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt; - &lt;a href=&#34;#food&#34;&gt;Food&lt;/a&gt; - &lt;a href=&#34;#stories&#34;&gt;Stories&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;drawing&#34;&gt;Drawing&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://i.ytimg.com/vi/ZUCHi_2ncPQ/maxresdefault.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Girl on mooon&lt;/a&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=ZUCHi_2ncPQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;motivation&#34;&gt;Motivation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;The motivation comes after work and not the other way around.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/bAmXn1fWnpE?t=6614&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jigarthanda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you want to go fast go alone, if you want to go far, go together&lt;/li&gt;
&lt;li&gt;When you have many things to do pick the most important and most interesting at that point to work and start working&lt;/li&gt;
&lt;li&gt;&amp;ldquo;He sought many things from the act of terror but one was notoriety. And that is why you will never hear me mention his name. He is a terrorist. He is a criminal. He is a extremist.  But he will when I speak be nameless&amp;rdquo; - Jacinda Ardern&lt;/li&gt;
&lt;li&gt;&amp;ldquo;You have to establish who you are in this world. Donot let others establish who you are&amp;rdquo; - Madonna to Rodman&lt;/li&gt;
&lt;li&gt;MJordan - Do as many things as possible and do not be lazy&lt;/li&gt;
&lt;li&gt;Pursue things which make you happy. Do not do things which make you sad&lt;/li&gt;
&lt;li&gt;Think of solutions of problems which are imminent. Do not think of imaginary problems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WPPPFqsECz0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Antidote to Dissatisfaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matthewjohnstone.com.au/courses/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I had a black dog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Caste - &lt;a href=&#34;https://www.youtube.com/watch?v=PZb4lGYkjrg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;India untouched&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In matters of conscience, tthe law of the majority has no place - M K G (Gandhi)&lt;/li&gt;
&lt;li&gt;I like the movie and not the maker. The maker can fail but the movie does not - Thiagarajan Kumararaja&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;movies&#34;&gt;Movies&lt;/h5&gt;
&lt;h5 id=&#34;series&#34;&gt;Series&lt;/h5&gt;
&lt;h6 id=&#34;amazon-prime&#34;&gt;Amazon Prime&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B0886H18DR/ref=atv_wl_hom_c_unkc_1_1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paatal Lok&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07TT23VHY/ref=atv_wl_hom_c_unkc_1_5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comicstaan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07WSD8XWF/ref=atv_wl_hom_c_unkc_1_15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Marvelous Mrs. Maisel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07SQB8LQ9/ref=atv_wl_hom_c_unkc_1_16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Undone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B079TKQV8H/ref=atv_wl_hom_c_unkc_1_22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Remix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B000WCT7M8/ref=atv_wl_hom_c_unkc_1_43&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;House&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07YCV6LCJ/ref=atv_wl_hom_c_unkc_1_8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goliath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B0064MGU98/ref=atv_wl_hom_c_unkc_1_30&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Good Wife&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07C2M2YPJ/ref=atv_wl_hom_c_unkc_1_23&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Killing Eve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B07GBGZW55/ref=atv_wl_hom_c_unkc_1_39&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Harmony with A R Rahman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/gp/video/detail/B074XLMGBQ/ref=atv_wl_hom_c_unkc_1_44&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mr. Bean&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;netflix&#34;&gt;Netflix&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.netflix.com/title/80212245&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dragon Prince&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.netflix.com/title/80165295&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Misaeng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.netflix.com/title/80153467&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unbelievable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.netflix.com/title/80192098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Money Heist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;songs&#34;&gt;Songs&lt;/h6&gt;
&lt;h5 id=&#34;short-stories-tamil&#34;&gt;Short Stories (Tamil)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.valaitamil.com/literature_short-story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;valaitamil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sramakrishnan.com/?cat=85&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sramakrishnan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://azhiyasudargal.blogspot.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;azhiyasudargal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sirukathaigal.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sirukathaigal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.quora.com/What-are-the-best-Tamil-sites-to-read-short-stories&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What-are-the-best-Tamil-sites-to-read-short-stories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ta.wikisource.org/wiki/%E0%AE%AE%E0%AF%81%E0%AE%A4%E0%AE%B1%E0%AF%8D_%E0%AE%AA%E0%AE%95%E0%AF%8D%E0%AE%95%E0%AE%AE%E0%AF%8D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.quora.com/What-are-the-best-short-stories-in-Tamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What-are-the-best-short-stories-in-Tamil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dostoevsky, Tolstoy - totally 7 books&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;food&#34;&gt;Food&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/iatPAjf5I_Y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intermittent fasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/uV_orGyIfGw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Myth about fruits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Almonds, Egg, Spinach,&lt;/li&gt;
&lt;li&gt;Carrot,&lt;/li&gt;
&lt;li&gt;Pudina, Garlic, Pattai/Lavangam(), Murungai illai&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;investment&#34;&gt;Investment&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fool.com/investing/how-to-invest-in-value-stocks.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;how-to-invest-in-value-stocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fool.com/investing/growth-investing-step-by-step-guide.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;growth-investing-step-by-step-guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fool.com/earnings-call-transcripts/?page=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;earnings-call-transcripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fool.com/investing/2020/03/08/3-top-tech-stocks-to-buy-right-now.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3-top-tech-stocks-to-buy-right-now&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;understanding a company performance for investment&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;cooking&#34;&gt;Cooking&lt;/h5&gt;
&lt;h6 id=&#34;sweet&#34;&gt;Sweet&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Y5v_65iXpY0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kesari&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5_cIM4PGd2U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paayasam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hkTAMlLcN2c&amp;amp;list=PL4mtHuMZdZjwUYR38YGrLMTlOgW-W3b6B&amp;amp;index=19&amp;amp;t=9s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Date Paayasam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wheat halwa: &lt;a href=&#34;https://www.youtube.com/watch?v=Gya1J-mGHBM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ast but may not be that good&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=oG7wglvAXeg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Looks too good. In Tamil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oHSNGbSQlDE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moong dal payasam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The great baking show : Netflix show: Cakes, Biscuits are a good to do bayesian experiments, cookies&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;non-veg&#34;&gt;Non-veg&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Biriyani Options: &lt;a href=&#34;https://www.youtube.com/watch?v=Sj6mM6IXAq8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chicken&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=uQqwj9um9_g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cooker Egg Biriyani&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=5VapbxkA_UA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;biriyani egg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JAVbJf3o3qk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prawn Fry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sUkt91x-cOY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fish Fry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=h_qsg8Gof4Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brinjal gravy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=upfu5nQB2ks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chicken tikka masala&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LtPu6Jd-ZhI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Butter masala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=X_idvqxRD3E&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spanish omlete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Frittata&lt;/li&gt;
&lt;li&gt;Egg roll wrapper and make samosa&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KatLap0bLmo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Egg malai masala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Uppu kari : mutton receipie - looks awesome&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;veg&#34;&gt;Veg&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=J3tCBVTgzLg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chinese dishes like noodles etc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aM_wiQFia2M&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paneer masala&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=eDOmZPQns8M&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mattar paneer - looks better. besan&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=_Um-jLO7_bM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paneer tikka masala&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=1mVP2TkjafI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paneer butter masala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://delightfulvegetarianrecipes.com/category/healthy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prashant recipes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;dishes-to-try&#34;&gt;Dishes to try&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Namma puli kuzhambu, Enna kathirikka kuzhambu, kerela ulli theyal, nadan mutta roast, Vadacurry, maharastrian misal thala, that is spiciest for dunking in pav bun&lt;/li&gt;
&lt;li&gt;Biriyani water-rice ratio - 1:1.25, or 1:1.50 - Hyderabadi style &amp;amp; Seeraga samba version. hyderabadi with salan &amp;amp; burani raitha. thalapakatti with salna &amp;amp; onion raitha. ambur with kathirikka &amp;amp; raitha. malabar biriyani they have it with oorga. Kolkata style &amp;amp; lucknowi.&lt;/li&gt;
&lt;li&gt;For chef, I like Jacques Pepin, Ottolenghi, Kenji LÃ³pez-Alt (he is science nerd turned chef).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sn9SqUkSFg4&amp;amp;list=PL_PgxS3FkP7ATPveBQ1yah7LDqysyzDCG&amp;amp;index=3&amp;amp;ab_channel=KQED&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=sn9SqUkSFg4&amp;list=PL_PgxS3FkP7ATPveBQ1yah7LDqysyzDCG&amp;index=3&amp;ab_channel=KQED&lt;/a&gt; - Jacques Pepin Cooking&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDFUoU1r0lJNMPfDNsBa3Z2vqacomOGx3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/playlist?list=PLDFUoU1r0lJNMPfDNsBa3Z2vqacomOGx3&lt;/a&gt; - Native receipies&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/CookingshookingIn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/c/CookingshookingIn&lt;/a&gt; - Thorough in his approach. His parathas are good and skills to make one&lt;/li&gt;
&lt;li&gt;Masala lab : Why cooking is done the way it is done? : &lt;a href=&#34;https://lifestyle.livemint.com/author/krish-ashok&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lifestyle.livemint.com/author/krish-ashok&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Book: &lt;a href=&#34;https://www.amazon.com/Family-Partition-Urvashi-Butalia-ebook/dp/B0756WQVKR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.amazon.com/Family-Partition-Urvashi-Butalia-ebook/dp/B0756WQVKR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCuMwOS-oxshrp6CxaaBBKWg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/channel/UCuMwOS-oxshrp6CxaaBBKWg&lt;/a&gt; - marwari kitchen&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWWUTc35N_j_bsC-gFWD0Hg/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/channel/UCWWUTc35N_j_bsC-gFWD0Hg/videos&lt;/a&gt; - IWC The Indian Wildlife Channel&lt;/li&gt;
&lt;/ul&gt;
&lt;h6 id=&#34;quotes&#34;&gt;Quotes&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Grandmother said, &amp;ldquo;You missed one ingredient. Write it down&amp;rdquo;. While I waited with a pen in hand , she said &amp;ldquo;Patience. Thats the ingredient you are missiong if you give anything enough time, it will turn out delicious. You can approximate all the other ingredients&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Names : Kathyayini, Taanumadhya, Dandan,&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;writing&#34;&gt;Writing:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;English speaking &lt;a href=&#34;https://www.youtube.com/watch?v=r_5K7cs24-8&amp;amp;list=PL4IJAxR6Bqq8vP0kEeQRgU6lUe4s4u2DW&amp;amp;index=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=QTJ02h7uiXs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learn English in 3 Hours - ALL You Need to Master English Conversation&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=6DiQ95hUUfI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;American accent&lt;/a&gt;, &lt;a href=&#34;http://dl.ueb.edu.vn/bitstream/1247/2927/1/American%20Accent%20Training.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;American Accent1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.writersbeat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writersbeat&lt;/a&gt;,&lt;a href=&#34;https://www.writingforums.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writingforums&lt;/a&gt;, &lt;a href=&#34;https://nybookeditors.com/2015/11/11-top-writing-communities-you-should-join-and-why/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nybookeditors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;books-to-read&#34;&gt;Books to read&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;communist manifesto, rights of man thomas paine&lt;/li&gt;
&lt;li&gt;THE ANNIHILATION OF CASTE - &lt;a href=&#34;file:///Users/kannappanjayakodinitthilan/Downloads/aoc_print_2004.pdf&#34;&gt;text&lt;/a&gt;, &lt;a href=&#34;https://soundcloud.com/freebuddhistaudio/sets/annihilation-of-caste&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;soundcloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;stories&#34;&gt;Stories&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;#my-brother&#34;&gt;My brother&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;my-brother&#34;&gt;My brother&lt;/h6&gt;
&lt;p&gt;As I stood there waiting for the bus/plane to arrive, my thouhgts dragged me to the past. Its been a while. It was one of those days. You have been thinking of saying something to your parents but did not have the confidence to talk about it. It was not different for a girl in here ninth grade.&lt;/p&gt;
&lt;p&gt;My artificial deadline was before the van arrives for the school that day. I want to vent it all out on my parents.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Kanna, are you out of your bed. Its getting late for school.&amp;rdquo; Amma/Mom gave morning alert. Mickey was licking my feet. She curled down at my feet after her morning ritual of dropping the newspaper at my fathers bedside. &amp;ldquo;Han, Ma. I am up.&amp;rdquo;  If not next she would be up in my room banging the door. This is her alarm. She is the final failsafe alarm when all alarms fail. Probably thats true about life too. Not sure. Parents.&lt;/p&gt;
&lt;p&gt;It all started few months back as a joke. As usual, I did not finish my lunch. &amp;ldquo;Yaen di. Innaikuma. Everyday this has becoma habit. If this carries on, I will sell you and get a new kid&amp;rdquo; &amp;ldquo;Try it. You never know. I may get better parents. or worse you may get even fussy brother&amp;rdquo; &amp;ldquo;Why brother? You do not like a sister.&amp;rdquo; &amp;ldquo;There could never be a better girl than me&amp;rdquo; &amp;ldquo;Fine. Atleast have dinner now. We will decide your auction date after a while&amp;rdquo;&lt;/p&gt;
&lt;p&gt;I slowly walked to the bathroom, reluctantly. One, I hate my science teacher and her so called surprise tests scheduled today. Two, I was not sure how I am going to share my thoughts with my parents. My brush poking its way around my mouth as I ruminate on these thoughts. Standing under the shower, I felt the water was cold today. Probably it would rain and the school would get cancelled. &amp;ldquo;Anju, Can you hurry up a little? Its getting late for my office&amp;rdquo; Appa knocked on my door. I can hear Mickey too scratching the door. Stepping out I thought I should go through all I wanted to talk once again as I dressed up. Today it PT class. I have to wear white today. As I stepped out of my room, amma had already prepared the lunch. Appa was also ready to leave for office. I sat on the couch, my throne. Appa was at my feet dusting my socks and cleaning my shoes. Amma came with dosa on the plate ready to stuff it in my mouth before I run away to catch the van. there was a 5 minute gap. The right time. &amp;ldquo;Appa, is it necessary to bring in a yonger brother&amp;rdquo; Appa paused a little &amp;ldquo;Yaen ma, why this question suddenly?&amp;rdquo; Amma, dipped a piece of dosa in sambar and as it dripped on the plate &amp;ldquo;Solluma. What are your thoughts?&amp;rdquo; &amp;ldquo;Illa ma. I am not feeling comfortable.&amp;rdquo; Appa dropped the shoes and took me on his lap and with a concerned face &amp;ldquo;Solluda. Why do you feel uncomfortable&amp;rdquo; &amp;ldquo;You said, two days back during dinner that you are planning to have sibling.&amp;rdquo; &amp;ldquo;Hmmm&amp;rdquo; Amma fed a socked dosa in my mounth. &amp;ldquo;Are you worried that you will have a competition.&amp;rdquo; &amp;ldquo;Do not worry da. I will clean both your shoes.&amp;rdquo; With a muffled voice of dosa in mouth &amp;ldquo;Unhun. Thats not a issue. Is it necesary to bring a new sibling?&amp;rdquo; &amp;ldquo;Ennada sollura. I am not able to understand what you are saying?&amp;rdquo; &amp;ldquo;Illa pa. Last year. On the way to Raju Sithappa place, it was raining heavily and we hit a dog by accident.&amp;rdquo; &amp;ldquo;Sari. Amma, Thats were we found mickey. She came limping towards us.&amp;rdquo; &amp;ldquo;Athan pa solluren. Last Sunday when we were sitting on the beach, there was a small kid selling Sundal. Why cannot he be my sibling?&amp;rdquo; Appa smiled. Amma fed the last piece of dosa and I rushed for the van.
My phone vibrated, breaking my thought process. &amp;ldquo;Hey Anju, where are you?&amp;rdquo; &amp;ldquo;Vaada. Just walk outside and walk towards the taxi stand.&amp;rdquo; If not for that day, I would not have met Koushik, my brother.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mental Health</title>
      <link>https://nitthilan.github.io/mental_health/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/mental_health/</guid>
      <description>&lt;h2 id=&#34;intention-of-this-page&#34;&gt;Intention of this page:&lt;/h2&gt;
&lt;p&gt;w my intention is to document the ways to handle so that it would be helpful for others. I feel everyone would have had it in some form or other at some level or other.
there are lot of myths and taboo which make people hesitant to share this.
I too have not visited one yet..but just for the experience plan to do so sometime. It would be useful to know
The more we understand the better it could be. Though my father warned to not go very deep in this pursuit since it may be counterproductive. I will anyway take it with a pinch of salt anyway&lt;/p&gt;
&lt;h2 id=&#34;hindrance-to-take-help-or-how-to-handle-it&#34;&gt;Hindrance to take help or How to handle it:&lt;/h2&gt;
&lt;p&gt;taboo in general to see a psychologist
we happy go to doctor for cold and fever. That is how much importance we give to mental health
if people are exposed to scientific literature at least in the popular sense, the stigma maybe less.
It&amp;rsquo;s more manageable. But I could handle it because I knew it was prevelant in my family. But it is still hard to discuss this in public. Like you say many hide this condition and until they come out and say it, you would not know.
if you say you have visited a Psychologist they assume you have been to a mental asylum.&lt;/p&gt;
&lt;h2 id=&#34;types-of-depression&#34;&gt;Types of depression:&lt;/h2&gt;
&lt;p&gt;Clinic anxiety and depression are not tough to quantity. They base it on by how long you go into the bad state or how severe when you go through the bad patch
Since my first experiences and experiences of other friends at that time, I think it is a continuum. It is not like normal people exist and those with mental health issues. It is an entire spectrum with varying colors, severity and meaning. Yup it&amp;rsquo;s a spectrum. Everyone has it in some level and it gets triggered based on environment and events&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Types of: Clinical depression, DSM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-does-it-manifest-how-long-does-it-last-what-happens-myths-and-misconceptions-can-you-explain-if-this-bad-phase-is-lasts-just-a-day-or-two&#34;&gt;How does it manifest? How long does it last? What happens? Myths and Misconceptions? can you explain if this bad phase is lasts just a day or two?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to identify that one has depression? What are the signs of depression?&lt;/li&gt;
&lt;li&gt;How to identify that one of your friends have depression?&lt;/li&gt;
&lt;li&gt;withdrawal from communication, withdrawal from light, withdrawl from daylight - harbinger symptoms&lt;/li&gt;
&lt;li&gt;How it is different from laziness and procrastination??&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, it is not like Jaundice where it comes ones and you never get it. It&amp;rsquo;s more like seasonal cold. You get it in cycles and phases.
when you are depressed you do things very weirdly like being very violent, you could hurt people etc.&lt;/p&gt;
&lt;p&gt;Nope, at least for me you feel you do not want to do anything &amp;hellip;just lay idle and stare. You would not realize the passage of time. Usually it takes few days but some extreme cases it may go on for weeks or months.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s the inability to do stuff which irritates you rather than doing things ðŸ˜‚&lt;/p&gt;
&lt;p&gt;Very true. The very fact that I knew something like this is possible helped me handle it better. Just that it is a passing phase and it&amp;rsquo;s not unique and permanent. Like cold it would just go away. You have to wait it out&lt;/p&gt;
&lt;p&gt;Nitt, how does the depression start? Is it an event, or a prolonged disappointment about something, is it a feeling of failure/not meeting your own expectations, personal relationship issues? Can you throw some light? I am curious to know.
This is an awesome question. Frankly, I have never spent time analysing how it started. Usually I get to know when I am into it say for few days or after it goes away where you mind feels cleared. It would be periods where you keep questioning all your actions. Your mind will go into endless arguments with yourself and eventually decide it&amp;rsquo;s better to do nothing than to take an action. You feel exhausted. But I have never been able to pinpoint and say when it starts. ðŸ¤”ðŸ¤” Let me think and if I find a reply will let you know.&lt;/p&gt;
&lt;p&gt;It depends. It&amp;rsquo;s usually any problem at hand at that point. There were periods where I felt bad that I couldn&amp;rsquo;t gain courage to go and purchase a Pillaiyar statue from a shop during Pillaiyar chathurthi. You get a feeling that the shop keeper would cheat you. What happens if you do not get the right statue? Will everyone like it? What happens if I pay more for a normal one? What size should I get? If it is too big what should we do? If it is small will it not be useful? Then you start questioning why am I thinking so much even for a small thing like this. And then it would exhaust you and drain your mind so much you may go ahead and then make the most silliest mistakes possible and then it would haunt you again. It&amp;rsquo;s weird but it happens. On a normal day this would not even be in your thought. You go get something and forget the whole issue. I may have exaggerated this a bit but roughly this is what happens&lt;/p&gt;
&lt;p&gt;Friend 1: From my personal evening experience, I see that any one nagging issue can seep into everything by playing in our mind and creating anxiety in us
Taking that nagging issue headlong usually solves it. But fear stops us .&lt;/p&gt;
&lt;p&gt;in general, i am less prone to frustration, depression etc since my nature has always been to not give too much importance to what others will think about me. also, i don&amp;rsquo;t delve much in the past , nor think too much about future. these help me in being able to attack present problems with assurance and certainty.&lt;/p&gt;
&lt;p&gt;what i am saying are for people for whom their biology is sound enough to be able to recognize these things properly. There can be medical conditions preventing us from thinking with clarity, making us more prone to anger, anxiety etc. Then it becomes difficult to just to self observations and bring things under control. and this issue has multiple layers and it maybe quite difficult to draw a line as to when to seek professional medical help&lt;/p&gt;
&lt;h2 id=&#34;how-to-cope-with-depression-tips-and-practices&#34;&gt;How to cope with Depression? Tips and Practices:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to handle depression?
&lt;ul&gt;
&lt;li&gt;how to get out of depression?&lt;/li&gt;
&lt;li&gt;how to document depression?&lt;/li&gt;
&lt;li&gt;how to get things done during depression?&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=njESlZa2b10&amp;amp;list=PL4mtHuMZdZjyeGP7SCP6s75_Ic9CN3s3y&amp;amp;index=8&amp;amp;t=21s&amp;amp;ab_channel=TEDxTalks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=njESlZa2b10&amp;list=PL4mtHuMZdZjyeGP7SCP6s75_Ic9CN3s3y&amp;index=8&amp;t=21s&amp;ab_channel=TEDxTalks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;second half of this: &lt;a href=&#34;https://www.youtube.com/watch?v=drv3BP0Fdi8&amp;amp;ab_channel=TEDxTalks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=drv3BP0Fdi8&amp;ab_channel=TEDxTalks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to treat a friend who is in depression?&lt;/li&gt;
&lt;li&gt;How to prevent it from happening or is that really a possibility?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do you do Pranayama on your non-depressed days?
Not suggesting meditation by the way. It is contraindicated in most mental health situations. Some amount of regular, gentle asana/vinyasa practice followed by a regimen that elongates the breath and makes it rhythmic (4:8, 6:12 are readily achievable with daily practice for healthy bodies) and could help manage overall wellbeing.&lt;/p&gt;
&lt;p&gt;Talking to someone usually helps &amp;hellip; Even if it is a stone statue in God&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thala, you need to write a journal. Just before sleeping, just write down bullet points of what you went through. Do this for two week. You can find some root reason.&lt;/p&gt;
&lt;p&gt;If the bad mood starts because you have some work to be completed but its not getting completed. You have some pointers here.&lt;/p&gt;
&lt;p&gt;check if you go through clinical depression or anxiety.&lt;/p&gt;
&lt;p&gt;So this bad patch of days - how long does it last until you bounce back?&lt;/p&gt;
&lt;p&gt;What makes you feel better after that? These are something, you can learn if you write journal.&lt;/p&gt;
&lt;p&gt;but we also need to understand that human mind is very much capable to creating problems on its own. So, while medical help will be needed in a subset , there will be many for whom understanding their own mind helps a lot. A good psychiatrist has to tread both worlds and give guidance&lt;/p&gt;
&lt;p&gt;Generally I have seen loneliness, boredom will automatically affect mental health easily.&lt;/p&gt;
&lt;p&gt;its considered very normal to keep thinking about past and future. we live in a mind-created world
we don&amp;rsquo;t realise that at all. Issues getting magnified by mind.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;adults generally put in a lot of energy to maintain their self-image. lots of time, effort goes into it. And also lot of fear, anxiety about loss of self-image. and this is also considered normal by families/socities. the self-image we have is mind-created&lt;/p&gt;
&lt;p&gt;lets say we have a button to stop thinking for a day. what a relief it will be for us - no need to keep up any identity. no need to think about future problems. just eat when hungry and sleep if needed. a button to kill ourselves and live free&lt;/p&gt;
&lt;p&gt;yes - what is considered valuable by society/family largely decides our intentions/actions&lt;/p&gt;
&lt;p&gt;this truth is staring at me for the last year or so.. the role mind plays.. i have become more observant i have had few minutes here and there of thoughtless observation and its changing my perspective of what i want to do in life&#39; i have seen issues there in mind just disappear once i observe it..&lt;/p&gt;
&lt;p&gt;the few minutes of thoughtlessness are generally filled with compassion/gratitude i can get a tiny peek at why great spiritual teachers have been obsessed with the mind and its behaviour&lt;/p&gt;
&lt;p&gt;Well, mind works with effort-reward. Life mostly doesn&amp;rsquo;t work like that. So if a jnani puts in all that effort (or perceives having done so) maybe their jnanam gets to be deficient. The reward in spiritual practice may not always look like what we expect.&lt;/p&gt;
&lt;p&gt;but for me personally, its generally been a change in perspective which has happened. And i am happy for it. actually i don&amp;rsquo;t have much expectations to  be honest, and i donno what to expect. the esoteric things that we hear about are as good as fairy tales&lt;/p&gt;
&lt;h2 id=&#34;good-demo-of-how-depression-looks-like&#34;&gt;Good demo of how depression looks like:&lt;/h2&gt;
&lt;p&gt;Episode 3 of Modern Love. It depicts a woman with bipolar. Anne Hathaway is brilliant in this character of course, she&amp;rsquo;s a natural especially in such roles.
Wrong demos - Movie three (tamil)
&lt;a href=&#34;https://www.youtube.com/watch?v=4IkTLiU5Q0k&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=4IkTLiU5Q0k&lt;/a&gt; - Daniel Fernandes&lt;/p&gt;
&lt;h1 id=&#34;list-of-tasks&#34;&gt;List of tasks:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;What is the science behind depression? What happens in the brain for a depressed person?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mbbMLOZjUYI&amp;amp;ab_channel=TED&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=mbbMLOZjUYI&amp;ab_channel=TED&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;youtube.com/watch?v=drv3BP0Fdi8&amp;amp;ab_channel=TEDxTalks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Playlist of depression related videos: &lt;a href=&#34;https://www.youtube.com/playlist?list=PL4mtHuMZdZjyeGP7SCP6s75_Ic9CN3s3y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/playlist?list=PL4mtHuMZdZjyeGP7SCP6s75_Ic9CN3s3y&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;quotes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;how come every other organ in your body can get sick and you get sympathy.. expect for the brain&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;First task is to know there is something like depression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know that it is very common but many people do not talk about it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Just knowing that there is something called depression and there are many with similar responses itself would reduce the burden&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to handle it initially? i.e when it happens first thats when people would get hit the worst?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.healthline.com/health/depression/best-videos-of-the-year#Youre-Not-Alone:-The-Truth-About-Depression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.healthline.com/health/depression/best-videos-of-the-year#Youre-Not-Alone:-The-Truth-About-Depression&lt;/a&gt; - A very good resource
&lt;a href=&#34;https://www.verywellmind.com/signs-you-may-be-a-perfectionist-3145233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.verywellmind.com/signs-you-may-be-a-perfectionist-3145233&lt;/a&gt; -&lt;/p&gt;
&lt;p&gt;chatter in the brain - J K - how to overcome chatterbox syndrome? - should never resist that and ignore ittune ur mind into something else. breathe in and breathe out and think about it. concentrate between eyes&lt;/p&gt;
&lt;p&gt;Discussion with friends:&lt;/p&gt;
&lt;p&gt;Links:
&lt;a href=&#34;https://www.facebook.com/RealDepressionProject&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/RealDepressionProject&lt;/a&gt;
&lt;a href=&#34;https://www.facebook.com/PoetDavidWhyte/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/PoetDavidWhyte/&lt;/a&gt;
&lt;a href=&#34;https://onbeing.org/programs/david-whyte-the-conversational-nature-of-reality/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://onbeing.org/programs/david-whyte-the-conversational-nature-of-reality/&lt;/a&gt;
&lt;a href=&#34;https://www.verywellmind.com/signs-you-may-be-a-perfectionist-3145233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.verywellmind.com/signs-you-may-be-a-perfectionist-3145233&lt;/a&gt;
tenpercent.com, &lt;a href=&#34;https://www.amazon.com/10-Happier-Self-Help-Actually-Works/dp/0062265431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.amazon.com/10-Happier-Self-Help-Actually-Works/dp/0062265431&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Philosophers: Ekhart Toulle. J K&lt;/p&gt;
&lt;h2 id=&#34;people-to-reach-out-for&#34;&gt;People to reach out for:&lt;/h2&gt;
&lt;p&gt;Campus Chaplin, Campus psycologist, Campus therapist
Psycologist/ psychiatrics - latter is a doctor who can prescribe medicines
Please find someone who will listen to you when you go through a bad phase.&lt;/p&gt;
&lt;p&gt;Also, when I say it runs in my family I have seen some of my relatives talk and take advice from my father and so know that there is something like this . My parents were very vocal about it and always encouraged me to not worry about it and have made me very comfortable. Amma is the more daring and cool of the two while appa would be precautious. Sister is like another Appa. So I had a very good system to deal with it&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I am in othe opposite end of the spectrum. I struggled a lot to learn the reality of the world. My parents somehow even now do not talk about stuff happening around them.Somehow they convey that to my wife, as she is keen to listen&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The world is inherently biased. Though the systems are available to make you believe that solutions are there to make it a balanced world. Democrazy does not make the world balanced since one side is inherently big. People are discriminated based on clolor. All people are not rewarded based on their performance. Some people are lucky based on oportunity, color, money, status. Black and White. Rich and poor. Lack of one does not compensate for the other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>To Infinity And Beyond</title>
      <link>https://nitthilan.github.io/to_infinity_and_beyond/_index_old/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nitthilan.github.io/to_infinity_and_beyond/_index_old/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;#about&#34;&gt;About&lt;/a&gt; - &lt;a href=&#34;#motion-capture-using-smpl-x-priors&#34;&gt;MoCap SMPL-X priors&lt;/a&gt; - &lt;a href=&#34;#neural-radiance-field-rendering-systems&#34;&gt;NeRF Rendering System&lt;/a&gt; - &lt;a href=&#34;#problems-working-on&#34;&gt;Problems&lt;/a&gt; - &lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;about&#34;&gt;About&lt;/h5&gt;
&lt;p&gt;With more and more AR and VR devices becoming ubiquitous and advancement in animation the animation, the difference between virtual and reality are getting blurred. This makes perception of reality and rendering it along with virtual scenes as the core problems. Being interested application of AI/ML in the area of 3D graphics and animation and having a background in video encoding for conferencing, transmission am working on two problems (a) Motion Capture from monocular videos dealing in perception and (b) Neural rendering of rigid and non-rigid bodies dealing in rendering.&lt;/p&gt;
&lt;p&gt;Further, the current advancements in deep learning in the area of GAN, Implicit representations, Transformers, GCN make them a invaluble tool that aid in solving these core problems. Anyone interested in the following problems can reach me &lt;a href=&#34;mailto:nitthilan@gmail.com&#34;&gt;nitthilan@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;motion-capture-using-smpl-x-priors-detailsproblems-working-on&#34;&gt;Motion Capture using SMPL-X priors &lt;a href=&#34;#problems-working-on&#34;&gt;(details)&lt;/a&gt;&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Using monocular video information
&lt;img src=&#34;mocap_parkor.gif&#34; alt=&#34;screen reader text&#34; title=&#34;VIBE&#34;&gt;&lt;/li&gt;
&lt;li&gt;Facial, finger and full Body pose extraction
&lt;img src=&#34;mocap_smplx.jpeg&#34; alt=&#34;screen reader text&#34; title=&#34;SMPLify-X&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 

















&lt;figure id=&#34;figure-a-caption&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mocap_smplx.jpeg&#34; data-caption=&#34;A caption&#34;&gt;


  &lt;img src=&#34;mocap_smplx.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    A caption
  &lt;/figcaption&gt;


&lt;/figure&gt;
 --&gt;
&lt;h6 id=&#34;neural-radiance-field-rendering-systems-detailsproblems-working-on&#34;&gt;Neural radiance field rendering systems &lt;a href=&#34;#problems-working-on&#34;&gt;(details)&lt;/a&gt;&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Fast training for rigid bodies
&lt;img src=&#34;NSVF.png&#34; alt=&#34;screen reader text&#34; title=&#34;NSVF&#34;&gt;
&lt;img src=&#34;neural_body.gif&#34; alt=&#34;screen reader text&#34; title=&#34;Neural Body&#34;&gt;&lt;/li&gt;
&lt;li&gt;Non-rigid (human models) pose, cloth and hair feature transfer
&lt;img src=&#34;nerf_controllable_features.gif&#34; alt=&#34;screen reader text&#34; title=&#34;ADGAN&#34;&gt; &lt;img src=&#34;nerf_pose_transfer.gif&#34; alt=&#34;screen reader text&#34; title=&#34;ADGAN&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;problems-working-on&#34;&gt;Problems Working On&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Shape and pose iteration for refinement of image based pose for the original dimension &lt;a href=&#34;https://github.com/nitthilan/video_pose_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;HMR (Human mesh recovery) use CNNs and so the images are resized thus losing its prediction precision.&lt;/li&gt;
&lt;li&gt;SMPLify-X based approches are iterative and do not reach global minima faster&lt;/li&gt;
&lt;li&gt;A hybrid approach which merges the both to get higher precision prediction while maintaining consistency across frames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multiresolution video pose estimation
&lt;ul&gt;
&lt;li&gt;A video has a large number of images and estimating pose for all frames would be huge&lt;/li&gt;
&lt;li&gt;Pose interpolation using transformers to reduce computation&lt;/li&gt;
&lt;li&gt;Transformer based approaches to use temporal consistency to get better prediction for occluded parts&lt;/li&gt;
&lt;li&gt;Identify non occluded frames and interpolate intermediate frames to estimate pose&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning pose using videos in the wild
&lt;ul&gt;
&lt;li&gt;Use 2D keypoint markers and temporal shape consistency to learn mocap information&lt;/li&gt;
&lt;li&gt;Estimate facial and finger pose estimation&lt;/li&gt;
&lt;li&gt;Estimate initial prediction using resized input images using neural network based approaches&lt;/li&gt;
&lt;li&gt;Apply SMPLify-X based iterative approaches to refine pose using full resolution images to get precise prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facial, hand and full pose control of deformable human neural avatars
&lt;ul&gt;
&lt;li&gt;Extract mocap information from driver video&lt;/li&gt;
&lt;li&gt;Learn neural body based avatars using SMPL-X&lt;/li&gt;
&lt;li&gt;Deform SMPL-X model using the driver video&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deformable neural cloth and hair styles extraction from video in wild
&lt;ul&gt;
&lt;li&gt;Use human parser to extract different clothing types and hair&lt;/li&gt;
&lt;li&gt;Learn neural models for each segment extracted earlier&lt;/li&gt;
&lt;li&gt;This helps in transfering information across different avatars&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-resolution neural training for both rigid and non-rigid objects
&lt;ul&gt;
&lt;li&gt;Active learing based appraches to identify subset of images which maximize learning&lt;/li&gt;
&lt;li&gt;Start with low resolution images and downsampled video&lt;/li&gt;
&lt;li&gt;Choose the training subset based on the rendered quality across different view&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Voxel based latent learning &lt;a href=&#34;https://github.com/nitthilan/kilonerf_modified&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Learn a pre-trained network which approximates simpler shapes ata voxel level&lt;/li&gt;
&lt;li&gt;Reduce training iteration time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neural sub-problems
&lt;ul&gt;
&lt;li&gt;Predicting occluded regions from known regions&lt;/li&gt;
&lt;li&gt;GAN based generator for neural body&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;references&#34;&gt;References:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mkocabas/VIBE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VIBE: Video Inference for Human Body Pose and Shape Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vchoutas/smplify-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMPLify-X&lt;/a&gt;, &lt;a href=&#34;https://smpl-x.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMPL-X&lt;/a&gt;, &lt;a href=&#34;https://smpl.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMPL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/frankmocap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FrankMocap: A Strong and Easy-to-use Single View 3D Hand+Body Pose Estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/akanazawa/hmr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-end Recovery of Human Shape and Pose&lt;/a&gt;, &lt;a href=&#34;https://github.com/nkolot/SPIN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPIN: SMPL oPtimization IN the loop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Illustrated Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeRF: Neural Radiance Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zju3dv/neuralbody&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/creiser/kilonerf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/NSVF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Sparse Voxel Fields (NSVF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/menyifang/ADGAN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Controllable Person Image Synthesis with Attribute-Decomposed GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/pifuhd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (CVPR 2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jonathan-hui.medium.com/debug-a-deep-learning-network-part-5-1123c20f960d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Debugging networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1812.07035.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Continuity of Rotation Representations in Neural Networks&lt;/a&gt;, &lt;a href=&#34;https://towardsdatascience.com/better-rotation-representations-for-accurate-pose-estimation-e890a7e1317f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tutorial blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
